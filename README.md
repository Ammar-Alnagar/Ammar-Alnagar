
# 👋 Ammar Alnagar  
**Senior GenAI Systems Engineer | Building lean, production-grade LLM infrastructure**

> *"If it’s not fast, maintainable, or measurable—it doesn’t ship."*

I design and deploy **scalable LLM inference systems** and **advanced RAG pipelines**—with a hard preference for **Rust**, **CUDA-aware Python**, and **minimalist architecture**. No fluff. No over-engineering. Just systems that scale under load.

🎓 **MSc in Artificial Intelligence**  
📍 Focus: **Inference engines (vLLM)**, **quantization**, **graph RAG**, **multi-agent orchestration**

---

## 🔧 What I Build

- **Efficient LLM Serving**: Optimized vLLM deployments, quantized models (GGUF/AWQ), distillation pipelines  
- **Graph-Based RAG**: Neo4j + dense retrieval + cross-encoder reranking  
- **Multi-Agent Systems**: LangGraph & CrewAI for task decomposition and tool use  
- **RLHF Infrastructure**: From reward modeling to PPO fine-tuning in specialized domains  

---

## 🛠 Core Stack

| Category       | Tools                                                                 |
|----------------|-----------------------------------------------------------------------|
| **Languages**  | Rust, Python, C++ (CUDA)                                             |
| **Inference**  | vLLM, Candle, Mistral.rs, TorchServe                    |
| **RAG/Agents** | LangGraph, CrewAI, Qdrant, Neo4j                                     |
| **Infra**      | Docker, AWS (EC2/S3), FastAPI, Prometheus/Grafana                     |
| **Optimization**| AWQ, GGUF, Unsloth, PEFT, Accelerate                                 |

> I avoid bloated abstractions. If a system can’t be audited in <500 lines of core logic, it’s already too complex.

---

## 🌐 Open Source

- [Hugging Face](https://huggingface.co/Daemontatox): Benchmarked quantized models, RAG configs, inference recipes  
- [GitHub](https://github.com/Ammar-Alnagar): Rust/Python tooling for lean LLM deployment  

I contribute **working systems**, not just notebooks.

---

## 🤝 Let’s Work Together

I’m open to:
- **Collaborating** on open-source LLM infrastructure (especially Rust-based)
- **Research** in efficient serving, distillation, or RAG evaluation
- **Production consulting** for teams shipping real LLM products

📧 **Reach out**: [ammaralnagar416@gmail.com](mailto:ammaralnagar416@gmail.com)  
*(No recruiters. No “quick calls.” Be specific about the problem.)*

---

<div align="center">

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat-square&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ammar-alnagar-393413201/)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-FFD21E?style=flat-square&logo=huggingface&logoColor=black)](https://huggingface.co/Daemontatox)
[![GitHub](https://img.shields.io/github/followers/Ammar-Alnagar?style=flat-square&logo=github)](https://github.com/Ammar-Alnagar)

*"Optimizing AI systems one inference at a time."*  
![Profile Views](https://komarev.com/ghpvc/?username=Ammar-Alnagar&color=blueviolet&style=flat-square)

</div>

---